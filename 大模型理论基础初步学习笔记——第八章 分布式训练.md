# 第八章 分布式训练

[本文GitHub地址](https://github.com/panda-like-bamboo/Study-CS324)https://github.com/panda-like-bamboo/Study-CS324


## 8.1 为什么分布式训练越来越流行

- **背景：** 深度学习广泛应用，模型规模急剧增大，如GPT-3参数量达1750亿。
- **限制：** 单一设备算力和内存受限，内存墙存在，提升硬件集成难度。
- **解决方案：** 分布式训练成为解决算力不足问题的必然选择。

### 8.1.1 内存墙

内存墙（Memory Wall）是指在计算机系统中，处理器的运算速度和内存的访问速度之间存在显著差异，导致处理器在等待从内存读取数据时的闲置状态。这种差异在多核、多线程处理器等现代计算机体系结构中变得更为明显。

在传统的计算机架构中，处理器的运算速度相对较快，而内存（主存）的访问速度相对较慢。由于处理器速度的不断提升，而内存速度的提升相对缓慢，导致了这种内存墙现象。

内存墙的存在意味着处理器在执行指令时，如果需要访问内存中的数据，可能会因为等待数据加载而陷入空闲状态，从而降低整体计算性能。这一现象尤其在大规模数据处理、复杂计算任务和需要频繁内存访问的应用中更为明显。

为了缓解内存墙的问题，计算机架构设计者采用了一些方法，如层次化存储结构（包括缓存层级）、预取技术、内存带宽优化等。这些方法旨在提高内存访问效率，使得处理器能更充分地利用计算资源而不过多受制于内存速度的限制。然而，内存墙问题仍然是计算机体系结构中需要不断优化和解决的挑战之一。

## 8.2 常见的并行策略

### 8.2.1 数据并行

- **定义：** 将输入数据 $x$ 在不同设备上进行切分，各设备拥有相同的模型参数 $w$。
- **流程：** 各设备计算部分输出，然后通过梯度同步（AllReduce）确保模型参数一致。
- **适用场景：** 数据集大、模型相对较小的情况，如ResNet50。
- **举例：** 在图像分类任务中，多个GPU同时处理不同批次的图像数据，梯度同步保持模型参数一致。

### 8.2.2 模型并行

- **定义：** 将模型参数 $w$ 在不同设备上进行切分，每个设备拥有完整的输入数据 $x$。
- **流程：** 需要进行数据广播，产生通信代价，各设备计算输出后进行合并。
- **适用场景：** 模型巨大、单个设备难以容纳的情况，如BERT。
- **举例：** 在自然语言处理任务中，将一个大型的语言模型切分到多个设备上，每个设备负责处理不同部分的文本数据。

### 8.2.3 流水并行

- **定义：** 将整个网络划分为多个阶段，每个阶段在不同设备上执行，形成计算“接力”。
- **流程：** 每个设备完成部分阶段的计算后，输出传递给下一个设备，形成流水线。
- **适用场景：** 当模型过大，无法在一个设备上存放时，可采用流水并行。
- **举例：** 在图像生成任务中，生成对抗网络（GAN）的生成器和判别器可以分别运行在不同设备上，通过流水线方式完成训练。

### 8.2.4 混合并行

- **定义：** 结合多种并行策略，根据具体情况选择不同的策略。
- **流程：** 可以在同一任务中同时采用数据并行、模型并行、流水并行等不同方式。
- **适用场景：** 对于复杂任务或特定模型，综合利用多种策略以优化性能。
- **举例：** GPT-3的训练中采用了流水并行和数据并行的结合，首先将模型划分为64个阶段，进行流水并行，然后每台主机内部进行数据并行。



## 参考：  
[1][datawhale讲义](https://github.com/datawhalechina/so-large-lm): https://github.com/datawhalechina/so-large-lm  
[2][CS324](https://stanford-cs324.github.io/winter2022/lectures/): https://stanford-cs324.github.io/winter2022/lectures/
[3][常见的分布式并行策略](https://docs.oneflow.org/master/parallelism/01_introduction.html)

