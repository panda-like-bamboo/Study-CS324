@[TOC](大模型理论基础初步学习笔记——第九,十章 大模型的危害)
# 大模型理论基础初步学习笔记——第九,十章 大模型的危害

[本文GitHub地址](https://github.com/panda-like-bamboo/Study-CS324)https://github.com/panda-like-bamboo/Study-CS324


## 9.1 本部分主要内容
本节主要涵盖了大型语言模型（Large Language Models，LLMs）可能引起的一系列危害，以及在其他领域中处理危害和安全性的一些高层次思想和方法。

## 9.2 危害

### 9.2.1 涉及到的领域和方法
- **Belmont报告和IRB**：关注尊重人、利益和公正的原则，建立了机构审查委员会（IRB）。
- **生物伦理学和CRISPR**：以基因编辑技术为例，社区规定禁止某些形式的人类基因编辑，对违规者实施强制性制度。
- **FDA和食品安全**：FDA是负责安全标准的监管机构，使用科学学科的理论来确定测试标准。
- **其他领域的伦理和安全性**：强调AI技术和LLMs的危害是相对较新的发展，与其他领域相比，缺乏强有力的理论，因此需要考虑广泛的社会政策来增加安全性。

### 9.2.2 总体原则
- 要牢记这些模型的能力与危害之间的密切关系，能力的潜力是这些模型被采用并引发危害的原因。
- 模型的能力提高通常导致更广泛的采用，进而导致更大的总体危害。

### 9.2.3 性能差异相关危害
- LLMs可以针对特定任务进行调整，性能差异表示模型在某些群体中的表现更好，而在其他群体中表现更差。
- 反馈循环可能放大差异，如果系统对某些用户不起作用，他们将不使用这些系统，导致未来系统表现出更大的差异。

### 9.2.4 社会偏见和刻板印象相关危害
- 社会偏见是一些概念（如科学）与某些群体（如男性）相对于其他群体（如女性）的系统关联。
- 刻板印象是社会偏见的一种特定普遍形式，是一种被广泛认同、过于简化且固定的关联。
- LLMs对数据中表现出反刻板印象的信息理解不足可能导致性能差异。

### 9.2.5 社会群体的识别和关注
- 文本中的社会群体可以基于作者/演讲者、读者/听众、以及文本中提到的人来识别。
- 保护属性是在美国不应作为决策基础的人口特征，例如种族、性别、性取向、宗教、年龄、国籍、残疾状况、外貌、社会经济地位等。
- 保护群体不是唯一重要的群体，但它们是一个起点，相关群体在文化和环境上具体而且具有特殊性。

### 9.2.6 性能差异和社会偏见案例
- **名字文物（Name Artifacts）**：LLMs在处理包含人名的文本时可能产生性能差异，例如在问题回答任务中，名字交换可能导致模型输出变化。
- **穆斯林与暴力关联**：LLMs可能强烈关联穆斯林与暴力，这种偏见持久存在。
- **StereoSet**：LLMs在处理涉及刻板印象的文本时可能表现出系统性倾向，且较大的模型倾向于更高的刻板印象得分。

### 9.2.7 测量
- 多种公平性度量存在，但很多不能同时最小化且无法捕捉利益相关者期望的结果。
- 测量偏见的设计决策可能显著改变结果，现有的LLMs基准测试也受到批评。
- 上游偏见测量并不总能可靠预测下游性能差异和实际危害。

### 9.2.8 其他考虑
- LLMs有潜力通过性能差异和社会偏见等方式造成危害。
- 理解这些危害的社会后果需要考虑涉及的社会群体及其地位，尤其是历史上受到歧视的缺乏权力的群体。
- 现有方法通常不足以显著减少/解决这些危害；社会技术方法可能是实质性减轻这些危害的必要途径。

## 9.3 行为伤害的介绍

之前的内容中，我们开始探讨由大型语言模型驱动的系统对使用者产生的负面影响，这些影响被称为“行为伤害”。这些伤害源自模型的行为，而非构造方法，包括性能差异和社会偏见刻

板印象。

- **性能差异：** 系统对不同人群的准确性存在差异，例如对年轻人或白人的准确性高于老年人或黑人。
  
- **社会偏见和刻板印象：** 系统的预测或生成的文本在目标概念与特定人群之间展现出强烈关联，可能对某些群体更为明显。

这些问题不仅仅存在于语言模型，也涉及到构造问题、数据隐私以及环境影响。

### 9.3.1 利益与伤害权衡的挑战

在考虑技术时，需要权衡带来的利益与伤害，但这是一项复杂任务，原因包括：

1. 利益与伤害难以量化。
2. 利益与伤害在人口中的分布不均匀，涉及到伦理问题。
3. 决策者在进行权衡时面临的权力问题，如Facebook或Google是否能单方面决定。

### 9.3.2 行为伤害的研究重要性

研究语言模型的伤害至关重要，因为这些模型具有新的、强大的能力，其广泛使用可能导致更多的伤害。

## 9.4 探索有毒性和假信息

在接下来的内容中，我们将探讨大型语言模型可能引起的另外两种行为伤害：**有毒性和假信息**。

- **有毒性：** 探讨大型语言模型在生成攻击性、有害或误导性内容方面的潜在伤害。
  
- **内容审查问题：** 引入内容审查问题，涉及到社交媒体平台对有害内容的抗争，以及使用AI进行内容审核的挑战。

下一部分将深入探讨有毒性，包括定义、评估工具（如Perspective API）以及实验结果。

## 9.5 虚假信息

**误导性信息**是指不论意图如何，被误导性地呈现为真实的错误信息。**虚假信息**则是有意为之地呈现错误或误导性信息以欺骗某一特定受众，其中存在对抗性质。虚假信息往往由恶意行为者创造，并通过社交媒体平台传播。例子包括石油公司否认气候变化、烟草公司否认尼古丁对健康的负面影响，以及一些阴谋论。

### 9.5.1 虚假信息战役的现状

恶意行为者在虚假信息战役中有特定目标，他们招募人力手动创建虚假信息。当前的虚假信息创造过程昂贵且慢，未来可能会更多地使用人工智能来进行虚假信息的创造。语言模型如GPT-3可能被用于更快速、更便宜地制造虚假信息。目前尚不清楚语言模型是否能够产生具有说服力的虚假信息，但存在经济动机促使恶意行为者使用这些模型。

### 9.5.2 虚假信息的经济学角度

目前尚不了解由语言模型驱动的虚假信息战役的严重性。关键问题是语言模型是否能够生成新颖、通顺、具有说服力的文本，以传达特定信息并针对目标人群。如果可以，使用GPT-3制造虚假信息的经济效益将增加。人工智能与人类结合的方法可能特别有效。

### 9.5.3 相关工作

GPT-3的研究表明生成的新闻文章与真实文章几乎无法区分。这表明语言模型可以生成新颖且通顺的文本，但是否具有说服力尚不清楚。一些研究指出GPT-3可能在网络极端化中发挥作用，创造具有意识形态一致性、互动性和规范性的环境。风险缓解措施包括保护大型语言模型、提高数字素养和检测模型。

## 9.6 内容审查

Meta（前Facebook）长期以来一直在打击有害内容，最近开始利用语言模型自动检测这类内容。"少量样本学习器"是Meta的最新内容审查模型，通过大量原始文本和历史数据进行训练。该模型通过简化任务为"蕴涵"来进行内容审查。少量样本学习器可以识别有害内容，例如鼓励对COVID疫苗表示怀疑或煽动暴力的言论。

## 参考：
[1][datawhale讲义](https://github.com/datawhalechina/so-large-lm): https://github.com/datawhalechina/so-large-lm  
[2][CS324](https://stanford-cs324.github.io/winter2022/lectures/): https://stanford-cs324.github.io/winter2022/lectures/