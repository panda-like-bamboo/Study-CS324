# 第十二章 大型语言模型的环境影响

[本文GitHub地址](https://github.com/panda-like-bamboo/Study-CS324)https://github.com/panda-like-bamboo/Study-CS324

## 12.1 介绍
本章探讨大型语言模型的环境影响，特别关注温室气体排放及其对气候变化的贡献。大模型对环境有一定的影响，但是在人类的总的排放中占比很小，比如谷歌使用了约12.2t千瓦时（4个最大模型的训练占比不到0.005%），相当于比特币挖矿支出的1/10；比如全球数据中心在2018年使用了2050亿千瓦时电力（总电力使用的1%）。

## 12.2 学习目标
### 12.2.1 全面理解
   - 12.2.1.1 探索大型语言模型的整体环境影响。
   - 12.2.1.2 计算特定语言模型训练时的排放影响。

### 12.2.2 意识与责任
   - 12.2.2.1 培养对监测和减轻负面环境影响的意识和个人责任感。

## 12.3 主要观点
### 12.3.1 气候变化
- **当前状况:**
  - 地球高于工业化前水平1.2°C。
  - 避免气候危机需要保持在1.5°C以下；当前轨迹预测几十年内将达到2.7°C增长。

- **大型语言模型:**
  - 训练对排放有贡献，例如，Strubell等估计了相当于5辆汽车寿命周期排放的626,000磅CO2eq。
  - DeepMind的Gopher报告称训练产生约380公吨CO2eq。

### 12.3.2 生命周期评估
- **哲学:**
  - 采用系统方法评估整体环境影响，而非仅仅是排放。

- **IT设备的生命周期:**
  - 包括生产、原材料提取、制造、运输、使用和末端处理。

- **考虑因素:**
  - 缺乏对GPU/TPU的生命周期评估。
  - 例如：法国一个CPU数据中心40%的温室气体排放来自生产阶段。

### 12.3.3 环境影响类别
- **温室气体排放:**
   - 气候变化的主要贡献者。
   - 根据能源来源的不同，例如化石燃料与绿色能源，排放量有所不同。

- **水足迹:**
   - 数据中心用于冷却的水。
   - 电力生成，特别是来自化石燃料的电力生成，消耗大量水资源。

- **人类毒性:**
   - 制造芯片时释放的化学物质对环境造成污染。

- **非生物资源枯竭:**
   - 用于制造电子设备的化石燃料和矿物质（如锂、钴）。

### 12.3.4 测量和估算
- **碳强度:**
  - 每千瓦时用电排放的碳量。
  - 依赖于能源来源（化石燃料与绿色能源）。

- **数据中心统计:**
  - 全球数据中心在2018年使用了2050亿千瓦时电力（总电力使用的1%）。
  - 0.5%的美国温室气体排放归因于数据中心。

### 12.3.5 估算训练模型的排放量
#### 12.3.5.1 ML CO2 Impact Calculator
- [ML CO2 Impact Calculator](https://mlco2.github.io/impact/) 是一个简单的工具，基于硬件、使用小时数、供应商和地区来估算模型训练的排放量。提供了对排放影响进行初步估算的便利途径。

#### 12.3.5.2 Strubell et al., 2018
这是一篇激发NLP社区对环境影响认识的重要论文。通过计算功耗，其模型估算了不同模型训练所产生的二氧化碳排放量。
- 平均功耗计算公式：
  $$\text{emissions} = R_{\text{power} \to \text{emit}} \text{PUE} (p_\text{cpu} + p_\text{gpu} + p_\text{dram})$$
- 典型值：
  - $\text{PUE}=1.58$（2018年全球数据中心平均值）
  - $R_{\text{power} \to \text{emit}}=0.954$（2018年平均排放量）

**结果示例**
- BERT-base（110M参数）：1438 lbs CO2eq（V100 GPU训练79.2小时）
- 神经结构搜索（213M参数）：626,155 lbs CO2eq（TPUv2训练10小时，总训练32623小时）
- 乘客往返纽约到旧金山航班：1984 lbs CO2eq
- 汽车生命周期：126,000 lbs CO2eq

#### 12.3.5.3 Patterson et al., 2021
这篇论文提供了一个简单的计算公式，估算了训练模型和推理所需的总排放量。
- 公式：
  $$\text{emissions} = R_{\text{power} \to \text{emit}} (\text{energy-train} + \text{queries} \cdot \text{energy-inference})$$
- NVIDIA发现80%的ML工作负载是推理而非训练。

**设计决策考虑**
- 模型架构：Transformer vs. Evolved Transformer
-

 处理器：NVIDIA's P100 vs. Google TPUs
- 数据中心：平均（1.58）vs. Google’s（1.11）
- 能源供应混合（如煤炭、水电）：平均（0.429 kg CO2eq / kWh）vs. Google’s（0.080 kg CO2eq / kWh）

**谷歌的估算模型**
- T5：86 MWh，47t CO2eq
- GShard：24 MWh，4.3t CO2eq
- Switch Transformer：179 MWh，59t CO2eq
- GPT3：1287 MWh，552t CO2eq

**反驳Strubell et al. (2019)的神经结构搜索估算**
- 小任务搜索高估了18.7倍
- 神经结构搜索仅进行一次，之后每个人都可以使用Evolved Transformer
- 排放量被高估了88倍

**重要观点**
- 测量优于在线计算，谷歌使用了约12.2t千瓦时（4个最大模型的训练占比不到0.005%），相当于比特币挖矿支出的1/10。

### 12.3.6 缓解策略
- **清洁能源数据中心:**
  - 在清洁能源数据中心上训练模型。

- **效率改进:**
  - 优化模型架构、训练流程和硬件以提高能效。

- **碳抵消:**
  - 有效性各异，需要谨慎考虑。

## 12.4 报告和进一步阅读
### 12.4.1 报告的重要性
   - 提高意识，推动对减少AI碳排放的激励。

### 12.4.2 工具和资源
   - ML CO2 Impact Calculator、Environment Impact Tracker、Carbon Tracker、CodeCarbon。
   - [Environment Impact Tracker](https://github.com/Breakend/experiment-impact-tracker)
- [Carbon Tracker](https://github.com/lfwa/carbontracker)
- [CodeCarbon](https://github.com/mlco2/codecarbon)

## 12.5 结论
虽然大型语言模型提供潜在好处，但它们的快速增长带来了环境挑战。缓解措施需要使用清洁能源、提高效率，并负责任地报告。好处和成本之间的权衡，尤其是它们的不均匀分布，使这成为一个复杂的问题。


## 参考：
[1][datawhale讲义](https://github.com/datawhalechina/so-large-lm): https://github.com/datawhalechina/so-large-lm  
[2][CS324](https://stanford-cs324.github.io/winter2022/lectures/): https://stanford-cs324.github.io/winter2022/lectures/

